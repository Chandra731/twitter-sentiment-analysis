{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ef_23fn-PLjE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3K3WOudoFJJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDb866_hPQc9"
      },
      "outputs": [],
      "source": [
        "# Create and Populate SQLite Database\n",
        "def create_and_populate_database(csv_file_path, db_name):\n",
        "    # Connect to SQLite\n",
        "    conn = sqlite3.connect(db_name)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Drop and create table\n",
        "    cursor.execute(\"DROP TABLE IF EXISTS tweets\")\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE tweets (\n",
        "            Sentiment INTEGER,\n",
        "            Id BIGINT,\n",
        "            Date TEXT,\n",
        "            Flag TEXT,\n",
        "            User TEXT,\n",
        "            Tweet TEXT\n",
        "        )\n",
        "    ''')\n",
        "    conn.commit()\n",
        "    print(\"tweets table has been reset.\")\n",
        "\n",
        "    # Load CSV data in chunks\n",
        "    chunk_size = 10000\n",
        "    for chunk in pd.read_csv(\n",
        "        csv_file_path,\n",
        "        delimiter=',',\n",
        "        chunksize=chunk_size,\n",
        "        encoding='ISO-8859-1',\n",
        "        names=['Sentiment', 'Id', 'Date', 'Flag', 'User', 'Tweet'],\n",
        "        header=0,\n",
        "        on_bad_lines='skip'\n",
        "    ):\n",
        "        # Normalize column names\n",
        "        chunk.columns = [col.lower().strip() for col in chunk.columns]\n",
        "\n",
        "        # Insert data into database\n",
        "        chunk.to_sql('tweets', conn, if_exists='append', index=False)\n",
        "\n",
        "    print(\"Data successfully loaded into tweets.db!\")\n",
        "    conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJy_uTN0PYO_"
      },
      "outputs": [],
      "source": [
        "# Data Retrieval\n",
        "def get_data_from_database(db_name='tweets.db', sample_size=100000):\n",
        "    conn = sqlite3.connect(db_name)\n",
        "    query = \"SELECT tweet, sentiment FROM tweets\"\n",
        "    data = pd.read_sql_query(query, conn)\n",
        "    conn.close()\n",
        "\n",
        "    # Sample the data\n",
        "    data = data.sample(n=sample_size, random_state=42)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUDSAz83Pa_F"
      },
      "outputs": [],
      "source": [
        "# Check data in the database\n",
        "db_path = '/content/drive/MyDrive/tweets.db'\n",
        "conn = sqlite3.connect(db_path)\n",
        "df = pd.read_sql_query(\"SELECT * FROM tweets LIMIT 10\", conn)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbhRNAzoPhYg"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing\n",
        "def data_process(data, tokenizer, max_len=128):\n",
        "    input_ids, attention_masks, labels = [], [], []\n",
        "\n",
        "    for _, row in data.iterrows():\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text=row['Tweet'],\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "        labels.append(0 if row['Sentiment'] == 0 else 1)\n",
        "\n",
        "    return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsIshSHy4bHO"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset Class\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_masks, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_masks = attention_masks\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attention_masks[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRvi64Q5Sy1h"
      },
      "outputs": [],
      "source": [
        "# BERT-LSTM Model Definition\n",
        "class BERT_LSTM(nn.Module):\n",
        "    def __init__(self, bert_model_name='bert-base-uncased', hidden_size=128, num_classes=2):\n",
        "        super(BERT_LSTM, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.lstm = nn.LSTM(self.bert.config.hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        with torch.no_grad():\n",
        "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        lstm_output, _ = self.lstm(bert_output.last_hidden_state)\n",
        "        pooled_output = lstm_output[:, -1, :]\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        logits = self.fc(dropout_output)\n",
        "        return self.softmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pim0eqLU4gGT"
      },
      "outputs": [],
      "source": [
        "# Save model Checkpoints\n",
        "def save_checkpoint(model, optimizer, epoch, checkpoint_path=\"checkpoint.pth\"):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epoch\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch + 1}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otnYrbb6TGWc"
      },
      "outputs": [],
      "source": [
        "# Load Model Checkpoint\n",
        "def load_checkpoint(model, optimizer, checkpoint_path=\"checkpoint.pth\"):\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch = checkpoint['epoch']\n",
        "        print(f\"Checkpoint loaded. Resuming from epoch {epoch + 1}.\")\n",
        "        return model, optimizer, epoch\n",
        "    return model, optimizer, -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC4fK8PGUBbG"
      },
      "outputs": [],
      "source": [
        "# Training Function\n",
        "def train_model(model, dataloader, optimizer, criterion, device, epoch, total_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{total_epochs}\", leave=False)\n",
        "    for input_ids, attention_masks, labels in progress_bar:\n",
        "        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_masks)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        progress_bar.set_postfix({'Loss': f\"{total_loss / (total + 1):.4f}\", 'Accuracy': f\"{correct / total:.4f}\"})\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrrQtaaz5JDE"
      },
      "outputs": [],
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_masks, labels in dataloader:\n",
        "            input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_masks)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    report = classification_report(all_labels, all_preds, output_dict=True)\n",
        "    return total_loss / len(dataloader), correct / total, report\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Step 1: Create and populate database\n",
        "    create_and_populate_database(CSV_FILE_PATH, DB_NAME)"
      ],
      "metadata": {
        "id": "JH2KzJTuFNNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvaBWNVXTBj5"
      },
      "outputs": [],
      "source": [
        "# Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    # Configurations\n",
        "    DB_NAME = '/content/drive/MyDrive/tweets.db'\n",
        "    BERT_MODEL_NAME = 'bert-base-uncased'\n",
        "    MAX_LEN = 128\n",
        "    BATCH_SIZE = 32\n",
        "    SAMPLE_SIZE = 100000\n",
        "    EPOCHS = 3\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    CHECKPOINT_PATH = \"checkpoint.pth\"\n",
        "\n",
        "    # Step 1: Retrieve and preprocess the data\n",
        "    data = get_data_from_database(DB_NAME, sample_size=SAMPLE_SIZE)\n",
        "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "    input_ids, attention_masks, labels = data_process(data, tokenizer, MAX_LEN)\n",
        "\n",
        "    # Step 2: Split data into train, validation, and test sets\n",
        "    train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
        "        input_ids, attention_masks, labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    train_dataset = SentimentDataset(train_inputs, train_masks, train_labels)\n",
        "    val_dataset = SentimentDataset(val_inputs,+ val_masks, val_labels)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Step 3: Define the model, optimizer, and loss function\n",
        "    model = BERT_LSTM(BERT_MODEL_NAME).to(DEVICE)\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Step 4: Load checkpoint if exists\n",
        "    model, optimizer, start_epoch = load_checkpoint(model, optimizer, CHECKPOINT_PATH)\n",
        "\n",
        "    # Step 5: Train the model\n",
        "    for epoch in range(start_epoch + 1, EPOCHS):\n",
        "        train_loss, train_acc = train_model(model, train_loader, optimizer, criterion, DEVICE, epoch, EPOCHS)\n",
        "        val_loss, val_acc, val_report = evaluate_model(model, val_loader, criterion, DEVICE)\n",
        "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Save checkpoint at the end of each epoch\n",
        "        save_checkpoint(model, optimizer, epoch, CHECKPOINT_PATH)\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), \"bert_lstm_sentiment_final.pth\")\n",
        "    print(\"Final model saved.\")"
      ]
    }
  ]
}