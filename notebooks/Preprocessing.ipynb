{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LsNnuP-2sse4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from termcolor import colored\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgU642IxtmTT",
        "outputId": "af23ee29-c9e7-4edf-bee7-6fda89a2aff1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import datasets\n",
        "print(\"Loading data\")\n",
        "train_data = pd.read_csv('/content/test.csv')\n",
        "test_data = pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo2yEk5KtqL4",
        "outputId": "ef438094-c98d-4549-f1a1-73b4970ab251"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize stopwords and remove negations\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "STOPWORDS.remove(\"not\")"
      ],
      "metadata": {
        "id": "jkiNY4UcuBME"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def clean_tweet(data):\n",
        "    # Initialize Lemmatizer and Stemmer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Define regular expressions\n",
        "    user_handle_pattern = r\"@[\\w]*\"\n",
        "    url_pattern = r\"(www\\.[^\\s]+)|(https?://[^\\s]+)\"\n",
        "    special_char_pattern = r\"[^a-zA-Z' ]\"\n",
        "    single_char_pattern = r\"(^| ).( |$)\"\n",
        "\n",
        "    # Define contractions\n",
        "    contractions = {\"n't\": \"not\"}\n",
        "\n",
        "    # Function to expand contractions in a tweet\n",
        "    def expand_contractions(tweet):\n",
        "        expanded_tweet = []\n",
        "        for word in tweet:\n",
        "            for contraction, replacement in contractions.items():\n",
        "                word = word.replace(contraction, replacement)\n",
        "            expanded_tweet.append(word)\n",
        "        return expanded_tweet\n",
        "\n",
        "    # Cleaning process\n",
        "    print(colored(\"Starting tweet preprocessing...\", \"yellow\"))\n",
        "\n",
        "    # Remove user handles\n",
        "    data['Clean_tweet'] = data['Tweet'].str.replace(user_handle_pattern, \"\", regex=True)\n",
        "\n",
        "    # Remove URLs\n",
        "    data['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(url_pattern), \"\", regex=True)\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    data['Clean_tweet'] = data['Clean_tweet'].str.replace(special_char_pattern, \"\", regex=True)\n",
        "\n",
        "    # Remove single characters\n",
        "    data['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(single_char_pattern), \" \", regex=True)\n",
        "\n",
        "    # Tokenize words\n",
        "    data['Clean_tweet'] = data['Clean_tweet'].str.lower().str.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [word for word in tweet if word not in STOPWORDS])\n",
        "\n",
        "    # Expand contractions\n",
        "    data['Clean_tweet'] = data['Clean_tweet'].apply(expand_contractions)\n",
        "\n",
        "    # Lemmatize and stem words\n",
        "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [lemmatizer.lemmatize(word) for word in tweet])\n",
        "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [stemmer.stem(word) for word in tweet])\n",
        "\n",
        "    # Recombine tokens into a single tweet string\n",
        "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: ' '.join(tweet))\n",
        "\n",
        "    print(colored(\"Tweet preprocessing complete!\", \"green\"))\n",
        "    return data"
      ],
      "metadata": {
        "id": "jCKee936uF0V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and save cleaned data\n",
        "print(colored(\"Processing train data...\", \"blue\"))\n",
        "train_data = clean_tweet(train_data)\n",
        "train_data.to_csv('/content/clean_train.csv', index=False)\n",
        "print(colored(\"Train data processed and saved to data/clean_train.csv\", \"green\"))\n",
        "\n",
        "print(colored(\"Processing test data...\", \"blue\"))\n",
        "test_data = clean_tweet(test_data)\n",
        "test_data.to_csv('/content/clean_test.csv', index=False)\n",
        "print(colored(\"Test data processed and saved to data/clean_test.csv\", \"green\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdig-ytDuLof",
        "outputId": "d018eb82-b337-455d-d598-96f998e0c309"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train data...\n",
            "Starting tweet preprocessing...\n",
            "Tweet preprocessing complete!\n",
            "Train data processed and saved to data/clean_train.csv\n",
            "Processing test data...\n",
            "Starting tweet preprocessing...\n",
            "Tweet preprocessing complete!\n",
            "Test data processed and saved to data/clean_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kola_b8IuQzm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}